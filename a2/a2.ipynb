{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 533V: Assignment 2 - Tabular Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em;\">Due Date: Wed Oct 6, 2021</p>\n",
    "<p style=\"font-size: 1.2em;\">100 Points Total (9% of final grade)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "* Your deliverable is this Jupyter Notebook. Submission will be done via Canvas.\n",
    "* For instructions on installing and running Jupyter Notebook: https://jupyter.org/install\n",
    "    * Start by cloning this repository: git clone git@github.com:UBCMOCCA/CPSC533V_2021W1.git\n",
    "    * Install Jupyter Notebook using either `conda install jupyter` or `pip install jupyter`\n",
    "    * Inside the `a2` folder, run `jupyter notebook` and a webpage should open in the browser\n",
    "    * If not, follow the instruction in terminal to launch an interactive session\n",
    "* If you use additional Python packages, please list them  as it will help with grading. \n",
    "* **We recommend working in groups of two**. List your names and student numbers below (if you use a different name on Canvas).\n",
    "\n",
    "<ul style=\"list-style-type: none; font-size: 1.2em;\">\n",
    "<li>Name (and student ID):</li>\n",
    "<li>Name (and student ID):</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Debugging Tips\n",
    "\n",
    "* Debugging in Jupyter Notebook can be using `pdb` or `ipdb`\n",
    "* Insert `import ipdb; ipdb.set_trace()` to where you want to set a breakpoint\n",
    "* See https://docs.python.org/3/library/pdb.html#debugger-commands for useful commands\n",
    "* Remember to quit out from an `ipdb` session, otherwise you may wonder why a code cell is taking forever to complete ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Tabular Q-Learning\n",
    "\n",
    "Tabular Q-learning is an RL algorithm for problems with discrete states and discrete actions. The algorithm is described in the class notes, which borrows the summary description from [Section 6.5](http://incompleteideas.net/book/RLbook2018.pdf#page=153) of Richard Sutton's RL book. In the tabular approach, the Q-value is represented as a lookup table. As discussed in class, Q-learning can further be extended to continuous states and discrete actions, leading to the [Atari DQN](https://arxiv.org/abs/1312.5602) / Deep Q-learning algorithm.  However, it is important and informative to first fully understand tabular Q-learning.\n",
    "\n",
    "Informally, Q-learning works as follows: The goal is to learn the optimal Q-function: \n",
    "`Q(s,a)`, which is the *value* of being at state `s` and taking action `a`.  Q tells you how well you expect to do, on average, from here on out, given that you act optimally.  Once the Q function is learned, choosing an optimal action is as simple as looping over all possible actions and choosing the one with the highest Q (optimal action $a^* = \\text{max}_a Q(s,a)$).  To learn Q, we initialize it arbitrarily and then iteratively refine it using the Bellman backup equation for Q functions, namely: \n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\text{max}_a Q(s', a) - Q(s,a)]$.\n",
    "Here, $r$ is the reward associated with with the transition from state s to s', and $\\alpha$ is a learning rate.\n",
    "\n",
    "In this assignment you will implement tabular Q-learning and apply it to CartPole â€“ an environment with a **continuous** state space.  To apply the tabular method, **you will need to discretize the CartPole state space** by dividing the state-space into bins.\n",
    "\n",
    "\n",
    "**Assignment goals:**\n",
    "- To become familiar with Python, NumPy, and OpenAI Gym\n",
    "- To understand and implement tabular Q-learning\n",
    "- To experiment tabular Q-learning on your implemention of discrete CartPole environment\n",
    "- (Optional) To develop further intuition regarding possible variations of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Deep reinforcement learning has generated impressive results for board games ([Go][go], [Chess/Shogi][chess]), video games ([Atari][atari], , [DOTA2][dota], [StarCraft II][scii]), [and][baoding] [robotic][rubix] [control][anymal] ([of][cassie] [course][mimic] ðŸ˜‰).  RL is beginning to work for an increasing range of tasks and capabilities.  At the same time, there are many [gaping holes][irpan] and [difficulties][amid] in applying these methods. Understanding deep RL is important if you wish to have a good grasp of the modern landscape of control methods.\n",
    "\n",
    "These next several assignments are designed to get you started with deep reinforcement learning, to give you a more close and personal understanding of the methods, and to provide you with a good starting point from which you can branch out into topics of interest. You will implement basic versions of some of the important fundamental algorithms in this space, including Q-learning, policy gradient, and search methods.\n",
    "\n",
    "We will only have time to cover a subset of methods and ideas in this space.\n",
    "If you want to dig deeper, we suggest following the links given on the course webpage.  Additionally we draw special attention to the [Sutton book](http://incompleteideas.net/book/RLbook2018.pdf) for RL fundamentals and in depth coverage, and OpenAI's [Spinning Up resources](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) for a concise intro to RL and deep RL concepts, as well as good comparisons and implementations of modern deep RL algorithms.\n",
    "\n",
    "\n",
    "[atari]: https://arxiv.org/abs/1312.5602\n",
    "[go]: https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[chess]:https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go \n",
    "[dota]: https://openai.com/blog/openai-five/\n",
    "[scii]: https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n",
    "[baoding]: https://bair.berkeley.edu/blog/2019/09/30/deep-dynamics/\n",
    "[rubix]: https://openai.com/blog/solving-rubiks-cube/\n",
    "[cassie]: https://www.cs.ubc.ca/~van/papers/2019-CORL-cassie/index.html\n",
    "[mimic]: https://www.cs.ubc.ca/~van/papers/2018-TOG-deepMimic/index.html\n",
    "[anymal]: https://arxiv.org/abs/1901.08652\n",
    "\n",
    "\n",
    "[irpan]: https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "[amid]: http://amid.fish/reproducing-deep-rl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# Only run if necessary\n",
    "!pip install gym\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Explore the CartPole environment [18 pts]\n",
    "\n",
    "Your first task is to familiarize yourself with the OpenAI gym interface and the [CartPole environment]( https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "by writing a simple hand-coded policy to try to solve it.  \n",
    "Read this brief introduction on [OpenAI Gym](https://gym.openai.com/docs/) to get started. \n",
    "The gym interface is very popular and you will see many algorithm implementations and \n",
    "custom environments that support it.  You may even want to use the API in your course projects, \n",
    "to define a custom environment for a task you want to solve.\n",
    "\n",
    "Below is some example code that runs a simple random policy.  You are to:\n",
    "- **run the code to see what it does**\n",
    "- **write code that chooses an action based on the observation**.  You will need to learn about the gym API and to read the CartPole documentation to figure out what the `action` and `obs` vectors mean for this environment. \n",
    "Your hand-coded policy can be arbitrary, and it should ideally do better than the random policy.  There is no single correct answer. The goal is to become familiar with `env`s.\n",
    "- **write code to print out the total reward gained by your policy in a single episode run**\n",
    "- **answer the short-response questions below** (see the TODOs for all of this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')  # you can also try LunarLander-v2, but make sure to change it back\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "# To find out what the observations mean, read the CartPole documentation.\n",
    "# Uncomment the lines below, or visit the source file: \n",
    "# https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "\n",
    "#cartpole = env.unwrapped\n",
    "#cartpole?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 [10pts] Complete the `TODO`s in the next code block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.1\n",
    "\n",
    "# Runs a single episode and render it\n",
    "# Try running this before editing anything\n",
    "\n",
    "obs = env.reset()  # get initial state/observation\n",
    "\n",
    "while True:\n",
    "    # TODO: replace this `action` with something that depends on `obs` \n",
    "    action = env.action_space.sample()  # random action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(0.1)  # so it doesn't render too quickly\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "# TODO: print out your total sum of rewards here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. [2pts] Describe the observation and action spaces of CartPole.  What does each of the values mean/do?**\n",
    "\n",
    "*Hint: Look at the full [source code here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) if you haven't already.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3. [2pts] What distribution is used to sample initial states? (see the `reset` function)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4. [2pts] What is the termination condition, which determines if the `env` is `done`?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5. [2pts] Briefly describe your policy.  What observation information does it use?  What score did you achieve (rough maximum and average)?  And how does it compare to the random policy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Discretize the environment [32 pts]\n",
    "\n",
    "Next, we need to discretize CartPole's continuous state space to work for tabular Q-learning.  While this is in part  a contrived usage of tabular methods, given the existence of other approaches that are designed to cope with continuous state-spaces, it is also interesting to consider whether tabular methods can be adapted more directly via discretization of the state into bins. Furthermore, tabular methods are simple, interpretabile, and can be proved to converge, and thus they still remain relevant.\n",
    "\n",
    "Your task is to discretize the state/observation space so that it is compatible with tabular Q-learning.  To do this:\n",
    "- **implement `obs_normalizer` to pass its test**\n",
    "- **implement `get_bins` to pass its test**\n",
    "- **then answer questions 2.3 and 2.4**\n",
    "\n",
    "[map]: https://arxiv.org/abs/1504.04909\n",
    "[qd]: https://quality-diversity.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 [15pts for passing test_normed]** Normalize observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.1\n",
    "\n",
    "def obs_normalizer(obs):\n",
    "    \"\"\"Normalize the observations between 0 and 1\n",
    "    \n",
    "    If the observation has extremely large bounds, then clip to a reasonable range before normalizing; \n",
    "    (-2,2) should work.  (It is ok if the solution is specific to CartPole)\n",
    "    \n",
    "    Args:\n",
    "        obs (np.ndarray): shape (4,) containing an observation from CartPole using the bound of the env\n",
    "    Returns:\n",
    "        normed (np.ndarray): shape (4,) where all elements are roughly uniformly mapped to the range [0, 1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # HINT: check out env.observation_space.high, env.observation_space.low\n",
    "    \n",
    "    # TODO: implement this function\n",
    "    raise NotImplementedError('TODO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST 2.1\n",
    "def test_normed():\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        obs, _, done, _ =  env.step(env.action_space.sample())\n",
    "        normed = obs_normalizer(obs) \n",
    "        assert np.all(normed >= 0.0) and np.all(normed <= 1.0), '{} are outside of (0,1)'.format(normed)\n",
    "        if done: break\n",
    "    env.close()\n",
    "    print('Passed!')\n",
    "test_normed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 [13pts for passing test_binned]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.2\n",
    "\n",
    "def get_bins(normed, num_bins):\n",
    "    \"\"\"Map normalized observations (0,1) to bin index values (0,num_bins-1)\n",
    "    \n",
    "    Args:\n",
    "        normed (np.ndarray): shape (4,) output from obs_normalizer\n",
    "        num_bins (int): how many bins to use\n",
    "    Returns:\n",
    "        binned (np.ndarray of type np.int): shape (4,) where all elements are values in range [0,num_bins-1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: implement this function\n",
    "    raise NotImplementedError('TODO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST 2.2\n",
    "obs = env.reset()\n",
    "env.close()\n",
    "\n",
    "def test_binned(num_bins):\n",
    "    normed = np.array([0.0, 0.2, 0.8, 1.0])\n",
    "    binned = get_bins(normed, num_bins)\n",
    "    assert np.all(binned >= 0) and np.all(binned < num_bins), '{} supposed to be between (0, {})'.format(binned, num_bins-1)\n",
    "    assert binned.dtype == np.int, \"You should also make sure to cast your answer to int using np.int() or arr.astype(np.int)\" \n",
    "    \n",
    "test_binned(5)\n",
    "test_binned(10)\n",
    "test_binned(50)\n",
    "print('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3. [2pts] If your state has 4 values and each is binned into N possible bins, how many bins are needed to represent all unique possible states)?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4. [2pts] After discretizing action space, is the dynamics deterministic or non-deterministic? Explain your answer in one to two sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Solve the env [30 pts] \n",
    "\n",
    "Using the pseudocode below and the functions you implemented above, implement tabular Q-learning and use it to solve CartPole.\n",
    "\n",
    "We provide setup code to initialize the Q-table and give examples of interfacing with it. Write the inner and outer loops to train your algorithm.  These training loops will be similar to those deep RL approaches, so get used to writing them!\n",
    "\n",
    "The algorithm (excerpted from Section 6.5 of [Sutton's book](http://incompleteideas.net/book/RLbook2018.pdf)) is given below:\n",
    "\n",
    "![Sutton RL](https://i.imgur.com/mdcWVRL.png)\n",
    "\n",
    "in summary:\n",
    "- **implement Q-learning using this pseudocode and the helper code**\n",
    "- **answer the questions below**\n",
    "- **run the suggested experiments and otherwise experiment with whatever interests you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup (see last few lines for how to use the Q-table)\n",
    "\n",
    "# hyper parameters. feel free to change these as desired and experiment with different values\n",
    "num_bins = 10\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "log_n = 1000\n",
    "# epsilon greedy\n",
    "eps = 0.05  #usage: action = optimal if np.random.rand() > eps else random\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Q-table initialized to zeros.  first 4 dims are state, last dim is for action (0,1) for left,right.\n",
    "Q = np.zeros([num_bins]*len(obs)+[env.action_space.n])\n",
    "\n",
    "# helper function to convert observation into a binned state so we can index into our Q-table\n",
    "obs2bin = lambda obs: tuple(get_bins(obs_normalizer(obs), num_bins=num_bins))\n",
    "\n",
    "s = obs2bin(obs)\n",
    "\n",
    "print('Shape of Q Table: ', Q.shape) # you can imagine why tabular learning does not scale very well\n",
    "print('Original obs {} --> binned {}'.format(obs, s))\n",
    "print('Value of Q Table at that obs/state value', Q[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 [25pts] Implement Q-learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.1\n",
    "\n",
    "# TODO: implement Q learning, following the pseudo-code above. \n",
    "#     - you can follow it almost exactly, but translating things for the gym api and our code used above\n",
    "#     - make sure to use e-greedy, where e = random about 0.05 percent of the time\n",
    "#     - make sure to do the S <-- S' step because it can be easy to forget\n",
    "#     - every log_n steps, you should render your environment and\n",
    "#       print out the average total episode rewards of the past log_n runs to monitor how your agent trains\n",
    "#      (your implementation should be able to break at least +150 average reward value, and you can use that \n",
    "#       as a breaking condition.  It make take several minutes to run depending on your computer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 [5pts] Plot the learning curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7f5e34ad8730>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q3.2\n",
    "\n",
    "# TODO: Plot the learning curve.\n",
    "#       Below is a snippet for generate a curve with upper and lower bounds.\n",
    "#       From your training loop above, save the episode rewards.\n",
    "#       Rerun the training code a few times to get min and max.\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(100)\n",
    "y = x ** 2\n",
    "plt.plot(y)\n",
    "\n",
    "lower_bound = 0.5*y\n",
    "upper_bound = 2.0*y\n",
    "plt.fill_between(x, lower_bound, upper_bound, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments [20 pts]\n",
    "\n",
    "Given a working algorithm, you will run a few experiments.  Either make a copy of your code above to modify, or make the modifications in a way that they can be commented out or switched between (with boolean flag if statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. [10pts] $\\epsilon$-greedy.**  How sensitive are the results to the value of $\\epsilon$?   First, write down your prediction of what would happen if $\\epsilon$ is set to various values, including for example [0, 0.05, 0.25, 0.5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the experiment and observe the impact on the algorithm.  Report the results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. [10pts] Design your own experiment.** Design a modification that you think would either increase or reduce performance.  A simple example (which you can use) is initializing the Q-table differently, and thinking about how this might alter performance. Write down your idea, what you think might happen, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A. Extensions (optional)\n",
    "\n",
    "- does the learning rate make a difference?\n",
    "- visualize the Q-table to see which values are being updated and not\n",
    "- design a better binning strategy that uses fewer bins for a better-performing policy\n",
    "- extend this approach to work on different environments (e.g., LunarLander-v2)\n",
    "- extend this approach to work on environments with continuous actions, by using a fixed set of discrete samples of the action space.  e.g., for Pendulum-v0\n",
    "- implement a simple deep learning version of this.  we will see next homework that DQN uses some tricks to make the neural network training more stable.  Experiment directly with simply replacing the Q-table with a Q-Network and train the Q-Network using gradient descent with `loss = (targets - Q(s,a))**2`, where `targets = stop_grad(R + gamma * maxa(Q(s,a))`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
