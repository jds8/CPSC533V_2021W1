{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 533V: Assignment 2 - Tabular Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em;\">Due Date: Wed Oct 6, 2021</p>\n",
    "<p style=\"font-size: 1.2em;\">100 Points Total (9% of final grade)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "* Your deliverable is this Jupyter Notebook. Submission will be done via Canvas.\n",
    "* For instructions on installing and running Jupyter Notebook: https://jupyter.org/install\n",
    "    * Start by cloning this repository: git clone git@github.com:UBCMOCCA/CPSC533V_2021W1.git\n",
    "    * Install Jupyter Notebook using either `conda install jupyter` or `pip install jupyter`\n",
    "    * Inside the `a2` folder, run `jupyter notebook` and a webpage should open in the browser\n",
    "    * If not, follow the instruction in terminal to launch an interactive session\n",
    "* If you use additional Python packages, please list them  as it will help with grading. \n",
    "* **We recommend working in groups of two**. List your names and student numbers below (if you use a different name on Canvas).\n",
    "\n",
    "<ul style=\"list-style-type: none; font-size: 1.2em;\">\n",
    "<li>Justice Sefas 38416574</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Debugging Tips\n",
    "\n",
    "* Debugging in Jupyter Notebook can be using `pdb` or `ipdb`\n",
    "* Insert `import ipdb; ipdb.set_trace()` to where you want to set a breakpoint\n",
    "* See https://docs.python.org/3/library/pdb.html#debugger-commands for useful commands\n",
    "* Remember to quit out from an `ipdb` session, otherwise you may wonder why a code cell is taking forever to complete ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Tabular Q-Learning\n",
    "\n",
    "Tabular Q-learning is an RL algorithm for problems with discrete states and discrete actions. The algorithm is described in the class notes, which borrows the summary description from [Section 6.5](http://incompleteideas.net/book/RLbook2018.pdf#page=153) of Richard Sutton's RL book. In the tabular approach, the Q-value is represented as a lookup table. As discussed in class, Q-learning can further be extended to continuous states and discrete actions, leading to the [Atari DQN](https://arxiv.org/abs/1312.5602) / Deep Q-learning algorithm.  However, it is important and informative to first fully understand tabular Q-learning.\n",
    "\n",
    "Informally, Q-learning works as follows: The goal is to learn the optimal Q-function: \n",
    "`Q(s,a)`, which is the *value* of being at state `s` and taking action `a`.  Q tells you how well you expect to do, on average, from here on out, given that you act optimally.  Once the Q function is learned, choosing an optimal action is as simple as looping over all possible actions and choosing the one with the highest Q (optimal action $a^* = \\text{max}_a Q(s,a)$).  To learn Q, we initialize it arbitrarily and then iteratively refine it using the Bellman backup equation for Q functions, namely: \n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\text{max}_a Q(s', a) - Q(s,a)]$.\n",
    "Here, $r$ is the reward associated with with the transition from state s to s', and $\\alpha$ is a learning rate.\n",
    "\n",
    "In this assignment you will implement tabular Q-learning and apply it to CartPole â€“ an environment with a **continuous** state space.  To apply the tabular method, **you will need to discretize the CartPole state space** by dividing the state-space into bins.\n",
    "\n",
    "\n",
    "**Assignment goals:**\n",
    "- To become familiar with Python, NumPy, and OpenAI Gym\n",
    "- To understand and implement tabular Q-learning\n",
    "- To experiment tabular Q-learning on your implemention of discrete CartPole environment\n",
    "- (Optional) To develop further intuition regarding possible variations of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Deep reinforcement learning has generated impressive results for board games ([Go][go], [Chess/Shogi][chess]), video games ([Atari][atari], , [DOTA2][dota], [StarCraft II][scii]), [and][baoding] [robotic][rubix] [control][anymal] ([of][cassie] [course][mimic] ðŸ˜‰).  RL is beginning to work for an increasing range of tasks and capabilities.  At the same time, there are many [gaping holes][irpan] and [difficulties][amid] in applying these methods. Understanding deep RL is important if you wish to have a good grasp of the modern landscape of control methods.\n",
    "\n",
    "These next several assignments are designed to get you started with deep reinforcement learning, to give you a more close and personal understanding of the methods, and to provide you with a good starting point from which you can branch out into topics of interest. You will implement basic versions of some of the important fundamental algorithms in this space, including Q-learning, policy gradient, and search methods.\n",
    "\n",
    "We will only have time to cover a subset of methods and ideas in this space.\n",
    "If you want to dig deeper, we suggest following the links given on the course webpage.  Additionally we draw special attention to the [Sutton book](http://incompleteideas.net/book/RLbook2018.pdf) for RL fundamentals and in depth coverage, and OpenAI's [Spinning Up resources](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) for a concise intro to RL and deep RL concepts, as well as good comparisons and implementations of modern deep RL algorithms.\n",
    "\n",
    "\n",
    "[atari]: https://arxiv.org/abs/1312.5602\n",
    "[go]: https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[chess]:https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go \n",
    "[dota]: https://openai.com/blog/openai-five/\n",
    "[scii]: https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n",
    "[baoding]: https://bair.berkeley.edu/blog/2019/09/30/deep-dynamics/\n",
    "[rubix]: https://openai.com/blog/solving-rubiks-cube/\n",
    "[cassie]: https://www.cs.ubc.ca/~van/papers/2019-CORL-cassie/index.html\n",
    "[mimic]: https://www.cs.ubc.ca/~van/papers/2018-TOG-deepMimic/index.html\n",
    "[anymal]: https://arxiv.org/abs/1901.08652\n",
    "\n",
    "\n",
    "[irpan]: https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "[amid]: http://amid.fish/reproducing-deep-rl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (from gym) (1.21.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (1.21.1)\n",
      "Requirement already satisfied: matplotlib in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (3.4.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (from matplotlib) (8.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (from matplotlib) (1.21.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: six in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pyglet in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (1.5.21)\n",
      "Collecting bottleneck\n",
      "  Downloading Bottleneck-1.3.2.tar.gz (88 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88 kB 3.1 MB/s eta 0:00:011\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/jsefas/driving-models/venv/lib/python3.8/site-packages (from bottleneck) (1.21.1)\n",
      "Building wheels for collected packages: bottleneck\n",
      "  Building wheel for bottleneck (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bottleneck: filename=Bottleneck-1.3.2-cp38-cp38-linux_x86_64.whl size=361727 sha256=d00da738930a47649cd4ac033d0ee5c87e34b35aaf18e806415d032fa6f84665\n",
      "  Stored in directory: /home/jsefas/.cache/pip/wheels/29/60/c9/98f744fb5c7d1ffb38d096318d9e873a08c0f5df07d6487626\n",
      "Successfully built bottleneck\n",
      "Installing collected packages: bottleneck\n",
      "Successfully installed bottleneck-1.3.2\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "# Only run if necessary\n",
    "!pip install gym\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install pyglet\n",
    "!pip install bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Explore the CartPole environment [18 pts]\n",
    "\n",
    "Your first task is to familiarize yourself with the OpenAI gym interface and the [CartPole environment]( https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "by writing a simple hand-coded policy to try to solve it.  \n",
    "Read this brief introduction on [OpenAI Gym](https://gym.openai.com/docs/) to get started. \n",
    "The gym interface is very popular and you will see many algorithm implementations and \n",
    "custom environments that support it.  You may even want to use the API in your course projects, \n",
    "to define a custom environment for a task you want to solve.\n",
    "\n",
    "Below is some example code that runs a simple random policy.  You are to:\n",
    "- **run the code to see what it does**\n",
    "- **write code that chooses an action based on the observation**.  You will need to learn about the gym API and to read the CartPole documentation to figure out what the `action` and `obs` vectors mean for this environment. \n",
    "Your hand-coded policy can be arbitrary, and it should ideally do better than the random policy.  There is no single correct answer. The goal is to become familiar with `env`s.\n",
    "- **write code to print out the total reward gained by your policy in a single episode run**\n",
    "- **answer the short-response questions below** (see the TODOs for all of this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')  # you can also try LunarLander-v2, but make sure to change it back\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "# To find out what the observations mean, read the CartPole documentation.\n",
    "# Uncomment the lines below, or visit the source file: \n",
    "# https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "\n",
    "#cartpole = env.unwrapped\n",
    "#cartpole?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 [10pts] Complete the `TODO`s in the next code block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222.0\n"
     ]
    }
   ],
   "source": [
    "# Q1.1\n",
    "\n",
    "# Runs a single episode and render it\n",
    "# Try running this before editing anything\n",
    "\n",
    "obs = env.reset()  # get initial state/observation\n",
    "rewards = 0\n",
    "\n",
    "while True:\n",
    "    # TODO: replace this `action` with something that depends on `obs` \n",
    "    action = int(obs[3] > 0)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    rewards += reward\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(0.1)  # so it doesn't render too quickly\n",
    "    if done: break\n",
    "env.close()\n",
    "print(rewards)\n",
    "# TODO: print out your total sum of rewards here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. [2pts] Describe the observation and action spaces of CartPole.  What does each of the values mean/do?**\n",
    "\n",
    "*Hint: Look at the full [source code here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) if you haven't already.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element of the observation space is a vector containing the lower bounds of cart position, velocity, pole angle, velocity; the second element is a vector containing their upper bounds, the third element contains the number of elements observed, the fourth element contains the type of the values in the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3. [2pts] What distribution is used to sample initial states? (see the `reset` function)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4. [2pts] What is the termination condition, which determines if the `env` is `done`?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pole Angle is more than 12 degrees or\n",
    "Cart Position is more than 2.4 (center of the cart reaches the edge of the display). or\n",
    "Episode length is greater than 200.\n",
    "\n",
    "Solved Requirements:\n",
    "Considered solved when the average return is greater than or equal to 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5. [2pts] Briefly describe your policy.  What observation information does it use?  What score did you achieve (rough maximum and average)?  And how does it compare to the random policy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy pushes the cart right if the angle of the pole with the normal to the cart is positive (i.e. the pole is falling right) and the policy pushes the cart left otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Discretize the environment [32 pts]\n",
    "\n",
    "Next, we need to discretize CartPole's continuous state space to work for tabular Q-learning.  While this is in part  a contrived usage of tabular methods, given the existence of other approaches that are designed to cope with continuous state-spaces, it is also interesting to consider whether tabular methods can be adapted more directly via discretization of the state into bins. Furthermore, tabular methods are simple, interpretabile, and can be proved to converge, and thus they still remain relevant.\n",
    "\n",
    "Your task is to discretize the state/observation space so that it is compatible with tabular Q-learning.  To do this:\n",
    "- **implement `obs_normalizer` to pass its test**\n",
    "- **implement `get_bins` to pass its test**\n",
    "- **then answer questions 2.3 and 2.4**\n",
    "\n",
    "[map]: https://arxiv.org/abs/1504.04909\n",
    "[qd]: https://quality-diversity.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 [15pts for passing test_normed]** Normalize observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.1\n",
    "\n",
    "obs_high = np.asarray([4.8, 2, 0.418, 2])\n",
    "obs_low = np.asarray([-4.8, -2, -0.418, -2])\n",
    "def obs_normalizer(obs):\n",
    "    \"\"\"Normalize the observations between 0 and 1\n",
    "    \n",
    "    If the observation has extremely large bounds, then clip to a reasonable range before normalizing; \n",
    "    (-2,2) should work.  (It is ok if the solution is specific to CartPole)\n",
    "    \n",
    "    Args:\n",
    "        obs (np.ndarray): shape (4,) containing an observation from CartPole using the bound of the env\n",
    "    Returns:\n",
    "        normed (np.ndarray): shape (4,) where all elements are roughly uniformly mapped to the range [0, 1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # HINT: check out env.observation_space.high, env.observation_space.low\n",
    "    \n",
    "    return (obs - obs_low) / (obs_high - obs_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "### TEST 2.1\n",
    "def test_normed():\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        obs, _, done, _ =  env.step(env.action_space.sample())\n",
    "        normed = obs_normalizer(obs) \n",
    "        assert np.all(normed >= 0.0) and np.all(normed <= 1.0), '{} are outside of (0,1)'.format(normed)\n",
    "        if done: break\n",
    "    env.close()\n",
    "    print('Passed!')\n",
    "test_normed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 [13pts for passing test_binned]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.2\n",
    "\n",
    "def get_bins(normed, num_bins):\n",
    "    \"\"\"Map normalized observations (0,1) to bin index values (0,num_bins-1)\n",
    "    \n",
    "    Args:\n",
    "        normed (np.ndarray): shape (4,) output from obs_normalizer\n",
    "        num_bins (int): how many bins to use\n",
    "    Returns:\n",
    "        binned (np.ndarray of type np.int): shape (4,) where all elements are values in range [0,num_bins-1]\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.digitize(normed, np.arange(0,1,1/num_bins))-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35479/3837541887.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  assert binned.dtype == np.int, \"You should also make sure to cast your answer to int using np.int() or arr.astype(np.int)\"\n"
     ]
    }
   ],
   "source": [
    "### TEST 2.2\n",
    "obs = env.reset()\n",
    "env.close()\n",
    "\n",
    "def test_binned(num_bins):\n",
    "    normed = np.array([0.0, 0.2, 0.8, 1.0])\n",
    "    binned = get_bins(normed, num_bins)\n",
    "    assert np.all(binned >= 0) and np.all(binned < num_bins), '{} supposed to be between (0, {})'.format(binned, num_bins-1)\n",
    "    assert binned.dtype == np.int, \"You should also make sure to cast your answer to int using np.int() or arr.astype(np.int)\" \n",
    "\n",
    "test_binned(5)\n",
    "test_binned(10)\n",
    "test_binned(50)\n",
    "print('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3. [2pts] If your state has 4 values and each is binned into N possible bins, how many bins are needed to represent all unique possible states)?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N^4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4. [2pts] After discretizing state space, is the dynamics deterministic or non-deterministic? Explain your answer in one to two sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the action deterministically sets the next state from the current state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Solve the env [30 pts] \n",
    "\n",
    "Using the pseudocode below and the functions you implemented above, implement tabular Q-learning and use it to solve CartPole.\n",
    "\n",
    "We provide setup code to initialize the Q-table and give examples of interfacing with it. Write the inner and outer loops to train your algorithm.  These training loops will be similar to those deep RL approaches, so get used to writing them!\n",
    "\n",
    "The algorithm (excerpted from Section 6.5 of [Sutton's book](http://incompleteideas.net/book/RLbook2018.pdf)) is given below:\n",
    "\n",
    "![Sutton RL](https://i.imgur.com/mdcWVRL.png)\n",
    "\n",
    "in summary:\n",
    "- **implement Q-learning using this pseudocode and the helper code**\n",
    "- **answer the questions below**\n",
    "- **run the suggested experiments and otherwise experiment with whatever interests you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q Table:  (10, 10, 10, 10, 2)\n",
      "Original obs [ 0.00636628  0.00072832 -0.0169109  -0.00852985] --> binned (5, 5, 4, 4)\n",
      "Value of Q Table at that obs/state value [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# setup (see last few lines for how to use the Q-table)\n",
    "\n",
    "# hyper parameters. feel free to change these as desired and experiment with different values\n",
    "num_bins = 10\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "log_n = 1000\n",
    "# epsilon greedy\n",
    "eps = 0.05  #usage: action = optimal if np.random.rand() > eps else random\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Q-table initialized to zeros.  first 4 dims are state, last dim is for action (0,1) for left,right.\n",
    "Q = np.zeros([num_bins]*len(obs)+[env.action_space.n])\n",
    "\n",
    "# helper function to convert observation into a binned state so we can index into our Q-table\n",
    "obs2bin = lambda obs: tuple(get_bins(obs_normalizer(obs), num_bins=num_bins))\n",
    "\n",
    "s = obs2bin(obs)\n",
    "\n",
    "print('Shape of Q Table: ', Q.shape) # you can imagine why tabular learning does not scale very well\n",
    "print('Original obs {} --> binned {}'.format(obs, s))\n",
    "print('Value of Q Table at that obs/state value', Q[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 [25pts] Implement Q-learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.682\n",
      "9.837\n",
      "9.899\n",
      "9.87\n",
      "9.914\n",
      "9.916\n",
      "26.348\n",
      "97.774\n"
     ]
    }
   ],
   "source": [
    "# Q3.1\n",
    "\n",
    "# TODO: implement Q learning, following the pseudo-code above. \n",
    "#     - you can follow it almost exactly, but translating things for the gym api and our code used above\n",
    "#     - make sure to use e-greedy, where e = random about 0.05 percent of the time\n",
    "#     - make sure to do the S <-- S' step because it can be easy to forget\n",
    "#     - every log_n steps, you should render your environment and\n",
    "#       print out the average total episode rewards of the past log_n runs to monitor how your agent trains\n",
    "#      (your implementation should be able to break at least +150 average reward value, and you can use that \n",
    "#       as a breaking condition.  It make take several minutes to run depending on your computer.)\n",
    "\n",
    "episode_rewards = []\n",
    "num_episode = -1\n",
    "prev_rewards = np.zeros(log_n)\n",
    "while True:\n",
    "    num_episode = num_episode + 1 if num_episode < log_n - 1 else 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "\n",
    "        action = np.argmax(Q[obs2bin(obs)]) if np.random.rand() > eps else env.action_space.sample()\n",
    "        \n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        rewards += reward\n",
    "\n",
    "        Q[obs2bin(obs)][action] += alpha * (reward + gamma*np.max(Q[obs2bin(new_obs)]) - Q[obs2bin(obs)][action])\n",
    "        \n",
    "        obs = new_obs\n",
    "        \n",
    "        # if num_episode == 0:\n",
    "        #    env.render()\n",
    "        #    time.sleep(0.1)  # so it doesn't render too quickly\n",
    "        if done: break\n",
    "            \n",
    "    prev_rewards[num_episode] = rewards\n",
    "    episode_rewards.append(rewards)\n",
    "    rewards = 0\n",
    "    \n",
    "    if np.mean(prev_rewards) > 150:\n",
    "        print(np.mean(prev_rewards))\n",
    "    if np.mean(prev_rewards) > 150: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 [5pts] Plot the learning curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7fa4769f29a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsP0lEQVR4nO3daXAk53kf8P/Tc+BeYA/sveSeEm+R9IqkLImiSB8UpRLpimzTcSLGkUPFlmM5TpUjxx9SrsoHOZWyLCUuuWhRNu34oEwrJiNLsSmSFi3ZorgUD4nngntjFzcwg7n7ePKh3270XDgWM5gD/18ViT4GPe80Zp955+m3n1dUFURE1F2sVjeAiIgaj8GdiKgLMbgTEXUhBncioi7E4E5E1IXirW4AAOzYsUMPHjzY6mYQEXWUF154YUZVR2vta4vgfvDgQZw4caLVzSAi6igicrbePqZliIi6EIM7EVEXYnAnIupCDO5ERF2IwZ2IqAsxuBMRdSEGdyKiLsTgTkTUhRjciYhapOR4TTs2gzsRUYvMZotNOzaDOxFRixRs9tyJiLqO7TK4ExF1HebciYi6UNFxm3ZsBnciohbJl9hzJyLqOgWbPXcioq7jqTbt2AzuREQt4HoKr3mxncGdiKgV8rbLnjsRUbdpZmAHGNyJiFpCmzdQBgCDOxFRS9hec6M7gzsRUQt4zbyaCgZ3IqKWaHJsX31wF5GYiLwoIl8z64dE5DkRGRORR0Ukabb3mPUxs/9gk9pORNSx2umC6qcBvB5Z/x0An1PVowDmAXzCbP8EgHmz/XPmcUREFNEWwV1E9gP4MIAvmXUBcCeAx8xDHgFwn1m+16zD7L/LPJ6IiIxm1nIHVt9z/z0AvwEgaM12AAuq6pj1CwD2meV9AM4DgNmfMo8vIyIPisgJETkxPT19ea0nIupQzawrA6wiuIvIRwBMqeoLjXxiVX1IVY+r6vHR0dFGHpqIqK2pKhYLzsoPXIf4Kh7zXgAfFZF7APQC2ALg8wBGRCRueuf7AYybx48DOADggojEAQwDmG14y4mIOlTedjGZLiAZb96AxRWPrKq/qar7VfUggPsBPK2qPw/gGQAfMw97AMDjZvkJsw6z/2nVJl85ICLqIBcX8kjl7aY+x3o+Nv4zgF8XkTH4OfWHzfaHAWw3238dwGfW10Qiou4yl7WbOgsTsLq0TEhV/wHAP5jlUwBuqfGYAoCfbkDbiIi6kuN5sN02GApJRESN0+SyMgAY3ImINpTtevBU0exLkQzuREQbyPUUijaqLUNEROsX9NrbovwAERE1hqrpvbPnTkTUWUqOVzXUMcixe6pNHykDMLgTETVc3nZRKJUPiSm5/roCyNvNLT0ArHGcOxERrcx2PcQqiuFmCg7shJ+OsR323ImIOk4wIibq/Hwe89kSVBWZInvuREQdx6sxGubSQh5xS9CTsFBocukBgD13IqKG87R6pqWZbAmpvA3P8y+4NhuDOxFRg6kqKvMyJcfDQs6GovnDIAEGdyKihio6LlSr70D1J+iwkSs1PyUDMOdORNRQk6kiFNU5d08V8zkbk+nChrSDPXciogYpOi7ytgvXqw7uqv68qRMpBncioo4ylS5isWDDdrWqrK9rgv1kurghbWFwJyJqkFzJRa7k4mIqHwbzgGeS8M2egSnA4E5E1CCupyg5Hk5OLsL1KnPu/s+NmlGawZ2IqEH8omAeskW3LOfu1cjBNxuDOxFRAy0W/NICTqTyo7MBJX4rMbgTETWI4ynSBRtA+R2qG91rBxjciYgapmi7Yc89Gs+zG1AorBKDOxFRgxQjNWOC3rrnKRby9oa3hcGdiKhB3BqpmILjomg3v1BYJQZ3IqIGiV5EDYO77SFvb8zY9igGdyKiBomObc+bafaCse8bjcGdiKhBnEjNgSBFo/DHvm80BnciogaJ9txdE+iDgmEbjcGdiKhBokE8yL/XqhC5ERjciYgaJBjjDiC8iOp6ipLL4E5E1JFcT5GJ3KxkR3ruvKBKRNShVMvrx6hZyRSdMP++kRjciYgaoGrOVPMzb7tMyxARdaqqi6Zm1XEVC9nShreHwZ2IqAGciq57sF6w3ap9G4HBnYioAapnXvLXc6WNH+MOrCK4i0iviHxPRF4WkVdF5LfN9kMi8pyIjInIoyKSNNt7zPqY2X+wya+BiKjlvIrgHlxQ3ag5UyutpudeBHCnqr4LwI0A7haR2wD8DoDPqepRAPMAPmEe/wkA82b758zjiIi6WmXOPYj1hRZUhARWEdzVlzGrCfOfArgTwGNm+yMA7jPL95p1mP13iYg0qsFERO3IrQjueduFamvqygCrzLmLSExEXgIwBeBJAG8DWFDVYMT+BQD7zPI+AOcBwOxPAdjewDYTEbWdyqHsBduFp9W5+I2yquCuqq6q3ghgP4BbAFy13icWkQdF5ISInJienl7v4YiIWkpRmXMHsiUHrQntaxwto6oLAJ4B8B4AIyISN7v2Axg3y+MADgCA2T8MYLbGsR5S1eOqenx0dPTyWk9E1CZqddAzBSe8sLrRVjNaZlRERsxyH4AfB/A6/CD/MfOwBwA8bpafMOsw+5/WVr06IqINUjlaBvDnVG1FRUgAiK/8EOwB8IiIxOB/GHxFVb8mIq8B+EsR+W8AXgTwsHn8wwD+VETGAMwBuL8J7SYiaiu1Yni+1JobmIBVBHdVfQXATTW2n4Kff6/cXgDw0w1pHRFRh6jVQ1/Il2oG/Y3AO1SJiFapVuolUGvPfNZu79EyRESbXb7kYi5XvwBYrSA+vVhoWc6dwZ2IaBUWCzZeODtft/dea9xIyfWYliEiamcXUwW8ObGIUp07TmtdOM0WW1NXBmBwJyJalYlUwcyHWju4t6r6Yz0M7kREqzBv8u2TqULN/a3KrdfD4E5EtALXU8yZ2ZRSebtq/0KutOxImlZgcCciWsHJqUWUHD8dUyu3PrVYbFkNmXoY3ImIVvDmxGK4XCv9kik6cFowCfZyVlN+gIhoU7sUybPXSq1nCk7Lblaqh8GdiLqG5ylEgEbOD1SwXRTspZEwlWkZ2/WQKTrhBdd2weBORF1jseDPHzTcn1jXcVQ1/IC4MJ8r6607FUMhc0UXU+kCFmpcaG0l5tyJqGuML+ThVE6JVKFgu1UBulKu5IZ3nM5ly4N2puggXVjadjGVR95u3Z2o9TC4E1FXyJdcTC0W4KouOywxnbeRXeGGo6LjYTpTBACcmc2W7UvlbcxnS8gU/W8Jp2eyKDrtdQMTwOBORF1iNlvE+EIe81kb2ZJT93HTmeKKwThvu5heLEJVka5It8xlS3jtYhonJ/0RNLOZYtv12gEGdyLqEjOZElJ5G2dnsxhfyNftvZ+fy4cjW6IXSgMlx8NCroSTkxmML+SrHqMKTKQLGJvKAPB7+e2IwZ2IusKF+RyKtofFgoNT09kwrVJpPldCyfGgquEIl2gAXyz4NdjHF/I4N5uDXWP8erbo4OKCX2um3coOBBjciagr2OYi6WLBxtnZHKYXawf3kuMhW3ThKcIbj2YiHwS5kouS46HkeHh7OlPnufyg7g+TbM+eO4dCElFXKJogm8o75mf10MSS46Fgu8gUHWQKTljhMVqaN2+7YW99JrP82PV0oXUzLa2EPXci6ngF2w3HmXvq96qD0SxRc9kSciUX87kS5nOlsOce9PLnsyUs5Gyk8qu7Ialde+0Ae+5E1AUm0wXkK4Y3LtS4Y/T0jD+scXw+j51DPWG+fNGMW88UHcxlixhfqF3Wt1I7DoEMMLgTUcebz1WnYGqlZfK235t3PA+T6SJiloS/77geTs1kkSu5VcMf62m3CTqimJYhoo43W2NkTL7kVQ2HDPLyuZKLs7PZ8G7VhXwJjqc4OblYM51Tz4X5/Poa3kTsuRNRx5upEdw9VaQLNkb6k+G24AKqqh/gPVXM5+xwCOViwcFaao6l2qxYWBR77kTU8Up1biQ6M5srW6/My9uuhrn5Sym/F76WYeuzWQZ3IqKmqTcc8VTFOHW7omBYKm+Hwx4zhdWnYwJtev8SAAZ3IuoCtaa+A4DZinHqpYq7TRcLNgpmxEu7lexdLwZ3Iup49UoAVAb9WnVizs/5qZu1XEjtBAzuRNRRtEYgr5dzj9Ztz5uyApUmzBR6OQZ3IqLWqSwJ4Lhe3bSM42mYj681ogbwyw0AS2ULNsr5uRy+8NTJmh9WjcDgTkQd5QfjC2UXUOeypWUvbK4U3IPf3ejqjl99cRxvTCxiIr26u2HXisGdiDrKXNYuC9QrDUd0PX9mppNTtSs8ttqzb0035bgM7kTUUWzXw3On58L1lUoAZIoOZjJFTKaa00Ner+fPzDfluLxDlYg6SjpvI523kS+56EvGyiarrsV2PaTydt28fKsIAAVwdOdgU47PnjsRdYzFgo1cyQ3nOAWAwgo9d08V421YA2b3cC8EwIPvP9yU4zO4E1HHWDDVH1WXxqWvlJYp2C7GF9ovuJccDzfsH4ZlraGYzRowuBNRx8iWloYrZooOPE/rjoIJjC8Uwnrt7aTkeuhJxJp2/BWDu4gcEJFnROQ1EXlVRD5ttm8TkSdF5KT5udVsFxH5goiMicgrInJz01pPRJtKdDq8+VwJ6YIdjlOv59xstuYk161mOx56483rX6/myA6A/6Sq1wC4DcCnROQaAJ8B8JSqHgPwlFkHgA8BOGb+exDAFxveaiLalKKTaBRsF6m8vWLxrlqTdrTamZksCo4HWUt94TVaMbir6iVV/b5ZXgTwOoB9AO4F8Ih52CMA7jPL9wL4E/V9F8CIiOxpdMOJaPMpRsoHZIvuiikZAG3Xa8/bLh5/+SKA2lMBNsqavhOIyEEANwF4DsAuVb1kdk0A2GWW9wE4H/m1C2Zb5bEeFJETInJiero5g/iJqLtE70wtOS7msu3XK1/JQ8+eCpdfGU817XlWHdxFZBDAXwP4NVVNR/epXxxhTR+PqvqQqh5X1eOjo6Nr+VUi2qSiVR1tVzGRar9RMGvx8duubNqxVxXcRSQBP7D/map+1WyeDNIt5ueU2T4O4EDk1/ebbURE65KLBPdsyUHBrl0NslPcenh70469mtEyAuBhAK+r6u9Gdj0B4AGz/ACAxyPbP25GzdwGIBVJ3xARXbbobEmq9SfpoNWVH3gvgH8N4Aci8pLZ9l8AfBbAV0TkEwDOAvgZs+/rAO4BMAYgB+AXGtlgItqcHNeD63lV29rRN354CUXHw303+pcbC+aO2gPb+jesDSsGd1X9NvwyCLXcVePxCuBT62wXEVEZ29WqkS/t2nN/a7K8AuVXvz+O6UyxbFx7k25MXTp+cw9PRNQYbjvPRr2CaTNksxAZyil1+8yNweBORB3BbbPx6vU43upSRVv6mluUl8GdiDpC0Vm+zEA7WCzY+P1n3g7Xgyn0rqiRa/+pm6pu/2koBnci6gjZFao/toMvf+dM2XpwSSARK0/BbOtPYqg30dS2MLgTUUfIFjd2Auu1qjUHa5CisV3F7i294fYbD4w0vT2ciYmIOkK7jowJFGpUp3RcRU/cnw0qERf8+9sP47VLaVy3b0vT28OeOxF1hJLTnmPaA9FJQ27YNwzAr4VzajqDS6kCkjELPYkYbrpia1OrQQYY3ImoI6x2FEqrzGaWKjzuHekD4H/b+PoPJwAAidjGhlsGdyLqCO0+zD3ojP+rW68Ib1DKFJ2wkqW1Ab31KAZ3IuoIbpvn3IP5XRNxCzET3aPfNnRthXPXjcGdiNqWRrrrK02E3Wr/fGoWgJ9+GelPAgBsZ6n9G/3ZxOBORG1rNruUx07lmzdrUSMM9fqDD3viFnpMDZlcZEJv3eDozuBORG3Jdj2cnc2F60Hao13tGurFtoEkLJEwuE8uLk0DyJ47ERH8SaTPz/nBPdcBE3OUXA9JMyImZgliInhzYjHcf/3+4Q1tD29iIqK2dGE+j3TB763P5+yad4C2k+BGJQAQkbIqlj9zfD/2DPdtaHvYcyeituR4ilzJxdRiAak2T8kA/k1WyTpj2Td6jDvAnjsRtSFVxUKuhILt4u2pbDi0sJ2VXA/JeO0gvmOwZ4Nbw+BORG2o6HiYSBWgClxK5esGzXZS2XPf0htHutC6YmcM7kTUdoqOFxYKm1oshqNPWilbdJApOuiJL41jD6iqn3OPBPfKKQE3GoM7EbWdaJGwfMlFscUjZfIlF1/69ulw/dN3HSvb73oKT1H2DcNu8eTdrf84JCKqUFkkrNUjZf7qhfPL7s+YWvP5yF20V273Z1/6Fzc3d8alethzJ6K2026jHucrRutkiw4Gevzw6XqK//PiOACgNxkLH/OT1+5Gpuhga0UKZ6Ow505EbaedYvs3X58Ml4MZlPKRiTkWcqXwwumeyGxLiZjVssAOMLgTURtK5y9vXPuJs3M4NZNpaFtevZgG4KdZDu8YAFCefnnshQvhcjsN2WRwJ6K2M59de5GwsakMvjM2i//78qVlH/fo8+fx/8wEGvX87++exf98+mTZvK333bgPfSbt8tUXx3F6JovFgo1C5OJvO4zqCbRPS4iIjOIap9Qbm8rgb3+wFNQ//9TJstRJYCJVwES6gDcnF6v2Rc1mS/AU4QiZ6820eX2JpZz6Ey9fxJe/cyZcf8/h7djegpuV6mFwJ6K2kymu7eaf8YV81baHnj0Fp2I4YvRxz56cxqPPn0fRdvFPb8/Adj28ejGFzz91supYH3znKACEPfdK7zmyHbcc2ramNjcbgzsRtZ3sGoN7vE6u+xuR9Iuq4ttjM+H6i+cWMJEu4JXxFJ4/M48fjKfwzdenah4nmNDaEsHHfmR/2b6t/QnccrC9AjvA4E5EbWgtNwAVHRcnzs4DAD51x5GyfadmsuHyubkcaplMFwAA/3hypmz7+47uAAB8+Po9Zdv3jfThU3ccwXX7tgAAtvQlVt3WjcRx7kTUVjJFp2wGppU88+Z0uByPWfjk7YcxNpXBU2+U98LrTdP39nS25vardg/hR67cWnNfPGahP+GHz/N1PjRajT13Imorqby9ppuYFs2wySAv3puI4bp9SxNjnJ31g3fBXGB98P2HVzxmXyIW3qRUzyvjCwA2foal1WJwJ6KWqVWnfSG3tmGQwQiV6/fVnunob166CAA4P+9fTO1NWPjUHUfwi+87VPPx97/7AH7+1itWfN4fu3oXgOq0TbtgcCeilhlfyFddPJ3JLB/cX7+ULpt4umC7GOlPhBc9A79651EAwLDJiZ82+XcRQTxm1eyZ335sB3Zt6V2x1w4AR0YH8em7juHozsEVH9sKDO5E1FTBBctKRcefZembr0/i1YsppEx6pbTMGPfnz8zh71+bxB/+42moyd2cnMrUnDxbRLBnuBdDvfG6F1ODO0r3DPtlA/qT3XMZsnteCRG1nVzJwWymhF2RmiuBl84t4IfjKdiu4sJ8Hvu39uHeG/ctG9yfOz0XLv/hP57Gz91yYNnn70vEcGEhHxb22jtc3o5f/sARTC0WMTrUg5NTi3jHro3thdc6L43C4E5ETfPqxTTcOlccJ9KFcEKLkuPh9EwW2aJTlnKpFD1W3nbx0vkFAP7IllqScavsw+LDN5Tnxy1LsNsE/Kt2b1n5BTVYUBa4GVZMy4jIl0VkSkR+GNm2TUSeFJGT5udWs11E5AsiMiYir4jIzU1rORG1tVTexqvjKZyZyaLolA9DnM+WcGG+/K5SVeCf3p6tOwxyJlOs2vb9cwsA/DtEazkdGed+9Z6htkq7JOMWtjWxauRqcu5/DODuim2fAfCUqh4D8JRZB4APAThm/nsQwBcb00wi6iSO6+Ht6QxSeQeT6SImUuV595fOL9RMv5ycWiyruBg1G7nQes/1u8v2DdYJ2u/aPxIu335sdLXN3xBb+hKwmlhFcsXgrqrPApir2HwvgEfM8iMA7ots/xP1fRfAiIi05zghImqasekMXruYhqcKTxULOTscZw4A2Tqpl+Wm0yuZu1bvuX43juwoz43XC5K3HvbLAhwZHUBvonZdmFYZ7Gluey73O8ouVQ1KsE0A2GWW9wGIzkd1wWyrqsEpIg/C793jiitWHlNKRJ3j3GyuLI3yxkQa2waSOLCtH7mSc1klfZ82d5zuHe4rC+Z3Xb2z7u9YIvjk7YfLJq5uB7uHe3HzFbXvfm2Udb9i9ccjrfkeLVV9SFWPq+rx0dH2+rpEROtzcipTdpfpRKqIs7P+cMR0fm3lBQI7Bv38dH9FZcZr9yx/IbQ3EWurSTRiluDozkHsGe5r6vNcbnCfDNIt5mdQxGEcQHRs0n6zjYg2Cc/TqsJfnipms0U4roe5bOmy5kjtTcSwZ7g3vFlp+2ASyZhVdfNSO0jEBIlYebss086+RAz9yRiSTZ7Y43KP/gSAB8zyAwAej2z/uBk1cxuAVCR9Q0SbQMn1agbvS6kCzs3l1lyrPZDO29jSu1SB8V/ecgU++YGV68S0wraBnqpqke8+tBWHRwfQk7BwYFvzhkAGVsy5i8hfALgDwA4RuQDgvwL4LICviMgnAJwF8DPm4V8HcA+AMQA5AL/QhDYTURsr1SnXmy+5mMmUMJetHtK4EtdTLBYcXLV7KWBabdhjFwG29ifRE7dgVQz6uWH/CIq2i1cupMo+pJplxeCuqj9XZ9ddNR6rAD613kYRUee6tFC73ADgT8KRuozJrxcLNhTAlr72GadeS5BLH+5L4NWLqbJ9PXELgz1x3P6OjbnG2N5niog6znLB+63JxTXPjwoAiwU/lbMRPd712NKbwFW7hzDQEy+7gWqkPxGO2Nmoi7vtNT6IiDreaxU91qhcya1bjqCS6ykupfy7WINx8ZUjZdrNtoFkWFFywIxj39qfaEnlSPbciahhpheLWLiMtEst/+uZMQB+Gd5gtqTVlOJtFRGEdWoAYPuAX2e+PxlvaoGwetr3TBFRxzk/n7usYY6VZiM3QD0bmdu03e4yjUrGLewwE4cAwGCvH173jvSVBf2NwrQMETXMudnGzCf6wrn5qm37R5p708969Vd88ATrV27vb8m1AgZ3ImqYqcX6I2XW4vVLi1XbDu0YaMixm6Wv4nrA1oEktg0kMdzfmovADO5E1DD50tpHwiznP5ip8oDWB3dLqu86jap1x+m7Doygr0WpJAZ3ImoIx/XgNSLhbly3dwssEdx3417cemgbRlrUAw68++BWfPxHD2Kot/alysGe6vZdv2+4ZUXLGNyJqCEu5+akWoqmNPBbkxkAwJXbB3Db4e0trSFzy6FtuO3wdmzpTeCavdWFyq7aPYQtNYJ+KwuWcbQMETVErs4kG2t1yUyofcc713YnpyWCvSO9SOXt8KanWnoSFlxX4Swz3r4nYZkCaIpr927Be4/uCPftGOxBfzKGgu1/U7lyez9uPby97cbgM7gTUUMs5BrTcz9lxrQfGV3bjT+3HNqGm64YwdhUBum8jTcnF2u26YPv3IlXL6ZxYT6Hod4E0nkbvYlYOJnIUG8c+7f24dq9w3jtUhofvKq8XvyxnYMYHexB3nbx2AsX8OPX7MJQG945y+BORA1Rb3al5RRtF8l4edneH4z7d7iupSTujsEk3nVgGL2JGK41aZNdw7144qWLZY87unMQ79g1hJ64hYFkDD0JCy+fT+HWw9vw7FvT2L2lF0d2DmLXUC8ObOuvWb1RRLB1IImtAA7uGGjLwA4wuBNRg9SqGeN5iqLjVQ0TBPwiYl/69mkAwAfeMYp37hpCznxADK7xTtQ73rkznPw6+KA4vGMAh0cHwm8CgD+naswSHB4dxBXb+nEpVcBkuogb94/g7GwWH7lhL1RX/8Fy+7EdKz+oRRjciWjdSo6HC/PlNzC5noYlBH7pA0fKAmY6b+OP/ulMuP6tt6bxrbemw/X3LxM0EzGB7S7ly0WA0aGeqseJCO6+bje++A9vQ9W/uzV6p2g85tdVf9/RHbAswXuP7FjzyJaR/uSaHr+ROFqGiNYtV3IwlV4qGXAplQ8DOwD8xfPnwmXX07LAXsu+Ze5GvffGfbjxipGwnvvVe7bULUvQE49hS28Cw30JfPTGvTV75EHqZWcL6r80E3vuRLRuFyM13D1VfOXEhbL9Czkb//z2LN5zZDueen0y3P4rHzyKmCXwPMXfvTqBt6b84Y+10jiAfyt/mAtX4IfjKdx0xciybXvHriFcvWcI2were/fdjMGdiNal6Lh46fxCuP7ka5Nl+3dt6cFkuojvnZnD0Z2DeH3CLy3ws8cPhOPALUvwoev3wH75Ig5s7as7y9LVkcmwr927BQM9cewcWr7H/d6jrR0j3yoM7kS0ZqmcDQgwkIzh5GSmbOq84ALmR27Ygz3DvYhbFr74rbcBAH/+vaX0TK1KiR991966zxmzBAe3L5UgGB3qWVXdls0Y2AHm3IlojVQVz56cxpOvTeLCfB5vTiyGFzjzthvOoXpkdBD9yTiScQu/GqkRAywfxOvZ2p8oS9eICHri7XXjUDthz52I1uTkVAZjJjdesF3MRGqvnzgzBwCo7CuLCH72+AE8euI8AD93vla1xpxTfQzuRLQmE6mli6fTi0uBvWi7+P65BQDAv3v/4arf2znUg2Tcwrv2D9fNqVeKDnvcsckuiK4XgzsRrcnZ2WzN7X9vLqRet3dLzdEuliX4pQ8cWfXzXLN3C96xawh/+8pFbOnzJ56m1WNwJ6JVU1XM16khE+TaP7DGgl+19CdjuP3YKPqSMfzUzfvhuop4i0rndioGdyJatYl0AW6daorZooNDOwYQt9YfhLcNJMPe/3I3NFF9/CgkolWrV7M9V3Iwn7Oxp0ETQV+7d7ghx9nMGNyJaNXOz+Vrbv/22AwAYPcab+GvNZnF9sEkju5cW7lfqsa0DBGtylS6gJNT1RNXp/J2OKF1ZQGvod44hvsSuDBf/aFwaMcAbtg/DNdTpAsO0gUbr5xP4fiV29ZU7pdqY3AnolU5M5tD0fZQdFz8zYsXMbVYwNV7tuDVi2kAwHuObA8LeO0d6cWxXUO4bu8wPFU888YUzszmULBdWCL40aPbcfMVW6t67jcf2IrBOnOU0trwLBLRqoxNZaCqePyli5gwU+EFgR0Abjm4DYBfC/1jP3KgLHDffd1uXEoV8PQbU9gz3It3m8dWWk05AVodBvcO5HkKT/05IOOWIGbJuutn2K4Hx1X0JqzLOpbjelAAjqsQQd0SrBppt/+8CsfzkIhZNWtpB49fa51t11NY0vi6Iq6nyBQcQIC+RKwqfeB6CoE/LFDEPx/JeO3XVo/teiZV4U//FhNBPLZ0q72qQtWvY76e16fqT6TheIp03kbcEnjqV3Us2C6yRRciQCJmoT8Zw0K+hHNzOVyK3MQU+JUP+uUFEjHBnVftrOqRiwj2jvThIzfsqfveoMbq+OD+xkQaqZyNkutBFXA8/2fedrGQs+GpP2yrNxEDFP590eqXFA3+8RUdF4mYhZglGOyJozcRQ9Hx51NcyNlI5W3Yrh+A4jELgz0xLORsFB3/H2FP3IIlgp6EBYGgPxlDT9xCyfWQL7nwFCg5Llz1g2DJ8RCPWUjEBHFLYJkA3ROPwRLAU/+2btdTWJagJ2bBsgT5koOS6weX4HUB/kWpmCXhPJB+QABiMQtxS5DO2/5r9RQ98VgYHCxLYJv2BMPbLBHELH8ig+CDI2YJXE/hehquD/bEYYmg5HjIlhyk8jYiTcJATwxDvQkM9sQhAuRLLvK2i1TOhuP5HwAAyn4nGbewtT+J3oSFouMhnbfDSYj7k34g7UvEYInAU/8Ytuu3SwRIxvxzbolgPluCAuhN+L8Tj1nImwmcEzEJg3Ku5JrfF8TMOQL8YKSq4Z2UtgnWBdsrGwrYn4yhvyeOou3CNu+lytcVnK8g3VC0/XPht8UKHy/iT3pRa6Lp6N+laM6JCDCQjCNmiXkvehARJGMWhvv8HnAqb6Po+O/BgZ44Bnv8iZ0XcjasyOtdrZcvpGAJ8Mnb/ck3So6HRGypc3HvjfuWLRPQzpNbdJvOD+6XFnF6pvYdc43n/6ObqbimVAqmF6s9kKAmx3NRaMx8wmHgDdpRqjHdWaBo198H+L02zwVsd/mZ7GczpWX3Z4t+z68erRFTSo6HyXR1rxDwg3Cu5GIBaztpK7VjvYJ2Lcf1FKm8XWcY4eraVuvvogpkipXzlvrvg+rt/uxH6Ugb1hjXkSk6ODubxbV7h8MPx+g3l11beln/pY3wkjQRrajkePjj75yBp8DNNSbHSMQEtx6unUen1uj4nrunfq9VVcN8oWeWbdeD5/lfrRNxCwL/q68lYpbFz1sGy2a/f1w/hWKbXrFnUhnRn54qBH6aIsjvxi0/PxoTgWN+V7GUJ1cs9VoVurRcsU39Ffi/gXCbmgcGGaaYJYhbVtgGN9ImjTxf5XGDtgTP4al/7HjMQsycFz8V4J8Q/3X45zImYmas97/WO67fW/RUTTvEtMs/7wBgO56fAnI9eF7k71Dx0xKYtvsTLpdcD1AgZlJYcXN9IUgtKZb+JuFr1OA8+GmPIL0Uvubo+b6Mv4nt+a81YVlIxv3ju+Z96OfcJXz+4DzEYxaSJgXjeB5st/o9Ff07+Kk8D5YFxC3/OYLUYaCsreH/1rA98vodV8P3dPA6gnRVT9zC6ZksXFW8/9gOjPQnsWMwCRFBumBj11Avbj28Dfu3stfeTjo6uP/Bt97GZ7/xRqubQbQpXLmtHzdfsRV3X7c7nBEpuMZD7aejg/vxK7fi7mt3I5W3YVlBz2+pF5gwFys9VdiOlvXAPLPgRbZFe4PBCIWg12VFevyWZZ4DAgXKevaO5486cU0vNm4JBALLgv8zMsIh+NYA8fctLcPsL/82IWZnsE11Kd/umDYE81F6ivJvJTW+oVQdXwSO64W9R1cVnuf3+IIeuef5223HPF9Mwh5s8M3B82Da5IXlWhMxv+eZNBeHg95p5c+g5yji53OD3m7QMw6OZ5lzVfUtzPxdgtcUjMZxzTe4svMdntfybSv9TeLmXDiefzHaCS40m28swftHsfS3cVwNC2sFvfDgvWBF/hZW5HUEF1sdd+lcRi/mSqRdQXurtkf+5sG+yvcSAMRjS9+4gtex9A3KhQIYHUzi+MHtZVPdMbC3r6YEdxG5G8DnAcQAfElVP9uM5zl+cBtmMkWcm8tV1YcOAmvwbg7SGYHgH0kQrFWXht0FIxcAhF+Fg2AapFiCYCcS+VptnsM1wdUS80GApYAe/E51qkTLLnBZkagbHd1iCcK0SWV6IhjNEry24LixSCGnYJRFIPjAsUxE8LzaqZzoB0pwDlTVtEn8ERNBtAhSFyawBa/bsswHrgl8qkvPC5NKCT5c/PTM0odq8OEVjBIyvxKe2+DDWUwjE5EPBc+8oOBvEE1vBX8Pc6qqloO3TdBm/0PfCs+7Yz7sLBNcgxGPYTgV/7mCNAywFNCD54myzIcH4AfcIK0XnB815yb8cNel87iUUlz6EAjfp0B4DsV0fICl92c8JmFbllJkS+fL8xS3Hd5ec2o8ak8ND+4iEgPw+wB+HMAFAM+LyBOq+lqjnwsA7r5uTzMOS0TU0ZoxWuYWAGOqekpVSwD+EsC9TXgeIiKqoxnBfR+A85H1C2ZbGRF5UEROiMiJ6enpJjSDiGjzatk4d1V9SFWPq+rx0dH1z9xCRERLmhHcxwEciKzvN9uIiGiDNCO4Pw/gmIgcEpEkgPsBPNGE5yEiojoaPlpGVR0R+RUAfwd/KOSXVfXVRj8PERHV15Rx7qr6dQBfb8axiYhoZSwcRkTUhRjciYi6kFTelt+SRohMAzi7hl/ZAWCmSc3pVDwn1XhOqvGclOv083GlqtYcS94WwX2tROSEqh5vdTvaCc9JNZ6Tajwn5br5fDAtQ0TUhRjciYi6UKcG94da3YA2xHNSjeekGs9Jua49Hx2ZcyciouV1as+diIiWweBORNSFOiq4i8jdIvKmiIyJyGda3Z5mEpEDIvKMiLwmIq+KyKfN9m0i8qSInDQ/t5rtIiJfMOfmFRG5OXKsB8zjT4rIA616TY0iIjEReVFEvmbWD4nIc+a1P2oK1kFEesz6mNl/MHKM3zTb3xSRn2zRS2kIERkRkcdE5A0ReV1E3rPZ3yci8h/Nv5sfishfiEjvpnuf+HNBtv9/8IuQvQ3gMIAkgJcBXNPqdjXx9e4BcLNZHgLwFoBrAPx3AJ8x2z8D4HfM8j0AvgF/WtHbADxntm8DcMr83GqWt7b69a3z3Pw6gD8H8DWz/hUA95vlPwDwS2b5lwH8gVm+H8CjZvka8/7pAXDIvK9irX5d6zgfjwD4RbOcBDCymd8n8CcHOg2gL/L++Deb7X3SST33TTV9n6peUtXvm+VFAK/Df9PeC/8fM8zP+8zyvQD+RH3fBTAiInsA/CSAJ1V1TlXnATwJ4O6NeyWNJSL7AXwYwJfMugC4E8Bj5iGV5yQ4V48BuMs8/l4Af6mqRVU9DWAM/vur44jIMIDbATwMAKpaUtUFbPL3CfyiiH0iEgfQD+ASNtn7pJOC+6qm7+tG5mviTQCeA7BLVS+ZXRMAdpnleuen287b7wH4DQCeWd8OYEFVHbMefX3hazf7U+bx3XRODgGYBvBHJlX1JREZwCZ+n6jqOID/AeAc/KCeAvACNtn7pJOC+6YkIoMA/hrAr6lqOrpP/e+Om2Ysq4h8BMCUqr7Q6ra0kTiAmwF8UVVvApCFn4YJbcL3yVb4ve5DAPYCGEBnfwu5LJ0U3Dfd9H0ikoAf2P9MVb9qNk+ar9EwP6fM9nrnp5vO23sBfFREzsBPy90J4PPwUwvB3ATR1xe+drN/GMAsuuucXABwQVWfM+uPwQ/2m/l98mMATqvqtKraAL4K/72zqd4nnRTcN9X0fSbn9zCA11X1dyO7ngAQjGR4AMDjke0fN6MhbgOQMl/L/w7AT4jIVtOj+QmzreOo6m+q6n5VPQj/7/+0qv48gGcAfMw8rPKcBOfqY+bxarbfb0ZJHAJwDMD3NuhlNJSqTgA4LyLvNJvuAvAaNvH7BH465jYR6Tf/joJzsrneJ62+oruW/+Bf6X8L/lXr32p1e5r8Wt8H/6v0KwBeMv/dAz8X+BSAkwC+CWCbebwA+H1zbn4A4HjkWP8W/sWgMQC/0OrX1qDzcweWRsschv+PbgzAXwHoMdt7zfqY2X848vu/Zc7VmwA+1OrXs85zcSOAE+a98jfwR7ts6vcJgN8G8AaAHwL4U/gjXjbV+4TlB4iIulAnpWWIiGiVGNyJiLoQgzsRURdicCci6kIM7kREXYjBnYioCzG4ExF1of8P/+mBGfBotjEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q3.2\n",
    "\n",
    "# TODO: Plot the learning curve.\n",
    "#       Below is a snippet for generate a curve with upper and lower bounds.\n",
    "#       From your training loop above, save the episode rewards.\n",
    "#       Rerun the training code a few times to get min and max.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from bottleneck import move_mean\n",
    "\n",
    "\n",
    "x = np.arange(len(episode_rewards))\n",
    "y = episode_rewards\n",
    "\n",
    "window_size = 100\n",
    "plt.plot(move_mean(episode_rewards, window_size))\n",
    "\n",
    "lower_bound = 0.5*np.asarray(move_mean(episode_rewards, window_size))\n",
    "upper_bound = 2.0*np.asarray(move_mean(episode_rewards, window_size))\n",
    "plt.fill_between(x, lower_bound, upper_bound, alpha=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments [20 pts]\n",
    "\n",
    "Given a working algorithm, you will run a few experiments.  Either make a copy of your code above to modify, or make the modifications in a way that they can be commented out or switched between (with boolean flag if statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. [10pts] $\\epsilon$-greedy.**  How sensitive are the results to the value of $\\epsilon$?   First, write down your prediction of what would happen if $\\epsilon$ is set to various values, including for example [0, 0.05, 0.25, 0.5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps == 0.0\n",
      "9.37\n",
      "9.377\n",
      "9.314\n",
      "9.354\n",
      "9.409\n",
      "9.372\n",
      "9.328\n",
      "9.348\n",
      "9.374\n",
      "9.368\n",
      "eps == 0.05\n",
      "10.099\n",
      "10.205\n",
      "10.299\n",
      "10.305\n",
      "10.263\n",
      "10.259\n",
      "10.332\n",
      "10.344\n",
      "10.367\n",
      "10.312\n",
      "eps == 0.25\n",
      "38.732\n",
      "eps == 0.5\n",
      "56.775\n",
      "eps == 0.8\n",
      "29.367\n",
      "33.293\n",
      "34.196\n",
      "34.136\n",
      "33.599\n",
      "34.369\n",
      "34.679\n",
      "34.439\n",
      "33.978\n",
      "35.31\n"
     ]
    }
   ],
   "source": [
    "# Q3.1\n",
    "\n",
    "# TODO: implement Q learning, following the pseudo-code above. \n",
    "#     - you can follow it almost exactly, but translating things for the gym api and our code used above\n",
    "#     - make sure to use e-greedy, where e = random about 0.05 percent of the time\n",
    "#     - make sure to do the S <-- S' step because it can be easy to forget\n",
    "#     - every log_n steps, you should render your environment and\n",
    "#       print out the average total episode rewards of the past log_n runs to monitor how your agent trains\n",
    "#      (your implementation should be able to break at least +150 average reward value, and you can use that \n",
    "#       as a breaking condition.  It make take several minutes to run depending on your computer.)\n",
    "\n",
    "\n",
    "for eps in [0.0, 0.05, 0.25, 0.5, 0.8]:\n",
    "    Q = np.zeros([num_bins]*len(obs)+[env.action_space.n])\n",
    "    print('eps == {}'.format(eps))\n",
    "    \n",
    "    episode_rewards = []\n",
    "    num_episode = -1\n",
    "    prev_rewards = np.zeros(log_n)\n",
    "    for t in range(10*log_n):\n",
    "        num_episode = num_episode + 1 if num_episode < log_n - 1 else 0\n",
    "\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "\n",
    "            action = np.argmax(Q[obs2bin(obs)]) if np.random.rand() > eps else env.action_space.sample()\n",
    "\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "\n",
    "            Q[obs2bin(obs)][action] += alpha * (reward + gamma*np.max(Q[obs2bin(new_obs)]) - Q[obs2bin(obs)][action])\n",
    "\n",
    "            obs = new_obs\n",
    "\n",
    "            # if num_episode == 0:\n",
    "            #    env.render()\n",
    "            #    time.sleep(0.1)  # so it doesn't render too quickly\n",
    "            if done: break\n",
    "\n",
    "        prev_rewards[num_episode] = rewards\n",
    "        episode_rewards.append(rewards)\n",
    "        rewards = 0\n",
    "        \n",
    "        if num_episode == log_n - 1:\n",
    "            print(np.mean(prev_rewards))\n",
    "        if np.mean(prev_rewards) > 75: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If eps == 0 then the algorithm would not improve very much since it would not explore new types of actions. It would get stuck in a local maximum and repeat the same actions whenever it's in a given state.\n",
    "\n",
    "eps == 0.05 probably performs well but not optimally since it does not explore the action space very much\n",
    "\n",
    "eps == 0.25 probably performs the best as it chooses the best option 3/4 of the time but explores quite often\n",
    "\n",
    "eps == 0.5 probably explores too much and uses bad actions too often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the experiment and observe the impact on the algorithm.  Report the results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that as epsilon increases, so does the speed at which the algorithm learns a good policy, i.e. one that averages more than a reward of 75 across episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. [10pts] Design your own experiment.** Design a modification that you think would either increase or reduce performance.  A simple example (which you can use) is initializing the Q-table differently, and thinking about how this might alter performance. Write down your idea, what you think might happen, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that increasing epsilon to 0.8 will make the algorithm degrade as it tries too many poor actions. I also believe that if we initialized the Q table with the output of a previous run, then it would perform better the second time around even with the same value of eps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps == 0.1\n",
      "iteration 0\n",
      "10.624\n",
      "10.642\n",
      "12.077\n",
      "12.386\n",
      "12.456\n",
      "iteration 1\n",
      "12.313\n",
      "12.394\n",
      "12.434\n",
      "21.648\n",
      "60.02\n"
     ]
    }
   ],
   "source": [
    "eps = 0.1\n",
    "\n",
    "Q = np.zeros([num_bins]*len(obs)+[env.action_space.n])\n",
    "print('eps == {}'.format(eps))\n",
    "\n",
    "for a in range(2): # the second iteration will have an already updated Q table from the first iteration\n",
    "    print('iteration {}'.format(a))\n",
    "    \n",
    "    episode_rewards = []\n",
    "    num_episode = -1\n",
    "    prev_rewards = np.zeros(log_n)\n",
    "    for t in range(5*log_n):\n",
    "        num_episode = num_episode + 1 if num_episode < log_n - 1 else 0\n",
    "\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "\n",
    "            action = np.argmax(Q[obs2bin(obs)]) if np.random.rand() > eps else env.action_space.sample()\n",
    "\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "\n",
    "            Q[obs2bin(obs)][action] += alpha * (reward + gamma*np.max(Q[obs2bin(new_obs)]) - Q[obs2bin(obs)][action])\n",
    "\n",
    "            obs = new_obs\n",
    "\n",
    "            # if num_episode == 0:\n",
    "            #    env.render()\n",
    "            #    time.sleep(0.1)  # so it doesn't render too quickly\n",
    "            if done: break\n",
    "\n",
    "        prev_rewards[num_episode] = rewards\n",
    "        episode_rewards.append(rewards)\n",
    "        rewards = 0\n",
    "\n",
    "        if num_episode == log_n - 1:\n",
    "            print(np.mean(prev_rewards))\n",
    "        if np.mean(prev_rewards) > 100: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did a test for eps = 0.8 to confirm that at some point eps is too large and the algorithm does not perform well. \n",
    "\n",
    "We can see that in the first iteration, the algorithm improves very slowly, but the second iteration improves faster and attains higher reward values given a better starting Q table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A. Extensions (optional)\n",
    "\n",
    "- does the learning rate make a difference?\n",
    "- visualize the Q-table to see which values are being updated and not\n",
    "- design a better binning strategy that uses fewer bins for a better-performing policy\n",
    "- extend this approach to work on different environments (e.g., LunarLander-v2)\n",
    "- extend this approach to work on environments with continuous actions, by using a fixed set of discrete samples of the action space.  e.g., for Pendulum-v0\n",
    "- implement a simple deep learning version of this.  we will see next homework that DQN uses some tricks to make the neural network training more stable.  Experiment directly with simply replacing the Q-table with a Q-Network and train the Q-Network using gradient descent with `loss = (targets - Q(s,a))**2`, where `targets = stop_grad(R + gamma * maxa(Q(s,a))`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
